[TOC]

### Scrapy框架五大组件

<img src="C:\Users\my\Desktop\mybook\图片\scrapy框架.png" style="zoom: 150%;" />

> 1. 引擎(Engine)               ：整个框架核心
> 2. 调度器(Scheduler)      ：维护请求队列
> 3. 下载器(Downloader)  ：获取响应对象 - 多线程
> 4. 爬虫文件(Spider)         ：数据解析提取
> 5. 项目管道(Pipeline)       ：数据入库处理
>
> ********************************************************************************************************************************************
>
> #下载器中间件：引擎->下载器，包装请求(随机代理等)
>
> #蜘蛛中间件：引擎->爬虫文件，可修改响应对象属性



### Scrapy爬虫工作流程

> 1. 爬虫项目启动
> 2. 由引擎向爬虫程序索要第一个要爬取的URL，交给调度器去入队列
> 3. 调度器处理请求后出队列，通过下载器中间件交给下载器去下载
> 4. 下载器得到响应对象后，通过蜘蛛中间件交给爬虫程序
> 5. 爬虫程序进行数据提取：
>    + 数据交给管道文件去入库处理
>    + 对于需要继续跟进的URL，再次交给调度器入队列，依次循环
>
> ```python
> Engine向Spider索要URL，交给Scheduler入队列
> Scheduler处理后出队列，通过Downloader Middlewares交给Downloader去下载
> Downloader得到响应后，通过Spider Middlewares交给Spider
> Spider数据提取：
> 	1.数据交给Pipeline处理
> 	2.需要跟进URL,继续交给Scheduler入队列，依次循环
> ```
>
> 



### Scrapy常用命令

```python
#1.创建爬虫项目
scrapy startproject 项目名

#2.创建爬虫文件
cd 项目文件夹
scrapy genspider 爬虫名 域名

#3.运行爬虫
scrapy crawl 爬虫名
```



### Scrapy项目目录结构

```python
 Baidu                                #项目文件夹
 ├─     Baidu                         #项目目录
 │          │  items.py               #定义数据结构
 │          │  middlewares.py         #中间件
 │          │  pipelines.py           #数据处理
 │          │  settings.py            #全局配置
 │          │  __init__.py
 │          │
 │          ├─spiders
 │            │  baidu.py             #爬虫文件
 │            │  __init__.py
 │          
 ├─     scrapy.cfg                    #项目基本配置文件
```



### 全局配置文件settings.py详解

```python
#1.定义User-Agent
USER_AGENT = 'Mozilla/5.0'
#2.是否遵循robots协议，一般设置为False
ROBOTSTXT_OBEY = False
#3.最大并发量，默认为16
CONCURRENT_REQUESTS = 32
#4.下载延迟时间
DOWNLOAD_DELAY = 1
#5.请求头，此处也可以添加User-Agent
DEFAULT_REQUEST_HEADERS={}
#6.添加管道
ITEM_PIPLINES={
	'项目目录名.pipelines.类名':300
}
```



### 创建爬虫项目步骤

```python
1. 新建：scrapy startproject 项目名
2. cd 项目文件夹
3. 新建爬虫文件：scrapy genspider 文件名 域名
4. 明确目标(items.py)
5. 写爬虫程序(文件名.py)
6. 管道文件(pipelines.py)
7. 全局配置(setting.py)
8. 运行爬虫 :scrapy crawl 爬虫名
    
1.scrapy startproject Tencent
2.cd Tencent
3.scrapy genspider tencent tencent.com
4.items.py(定义爬取数据结构)
import scrapy
class Tencentitem(scrapy.item):
    job_name = scrapy.Field()
5.tencent.py(写爬虫文件)
import scrapy
class TencentSpider(scrapy.Spider):
    name = 'tencent'
    allowed_domains = ['tencent.com']
    start_urls = ['http://tencent.com/']
    def parse(self,response):
        pass
6.pipelines.py(处理)
class TencentPipeline(object):
    def process_item(self,item,spider):
        return item
7.setting.py(全局配置)
ROBOTSTXT_OBEY = False
DEFAULT_REQUEST_HEADERS={}
ITEM_PIPLINES={
	'项目目录名.pipelines.类名':300
}
8.终端：scrapy crawl tencent
```



### pycharm运行爬虫项目

```python
1. 创建begin.py(和scrapy.cfg文件同目录)
2. begin.py中内容：
	from scrapy import cmdline
	cndline.excute('scrapy crawl maoyan'.split())
```



### 响应对象属性及方法

```python
#属性
response.text :获取响应内容 - 字符串
response.body :获取bytes数据类型
response.xpath('')

#response.xpath('')调用方法
1.结果：列表，元素为选择器对象
#<selector xpath='//article' data=''>
2.extract():提取文本内容，将列表中所有元素序列化为Unicode字符串
3.extract_first():提取列表中第1个文本内容
4.get():提取列表中第1个文本内容
```



### 爬虫项目启动方式

+ #### 方式一

```python
从爬虫文件(spider)的start_urls变量中遍历URL地址，把下载器返回的响应对象(response)交给爬虫文件的parse()函数处理
# start_urls = ['http://www.baidu.com/']
```

+ #### 方式二

```python
重写start_requests()方法，从此方法中获取URL，交给指定的callback解析函数处理

1.去掉start_urls变量
2.def start_requests(self):
	#生成要爬取的URL地址，利用scrapy.Request()方法交给调度器
	
yield scrapy.Request(
                url = url,
                callback=self.parse_html
            )	
```



### 知识点汇总

+ ##### 节点对象.xpath('')

```python
1. 列表，元素为选择器 ['<selector data='A'>']
2. 列表.extract() : 序列化列表中所有选择器为Unicode字符串['A','B','C']
3. 列表.extract_first() 或者 get() :获取列表中第1个序列化的元素(字符串)
```

+ ##### pipelines.py中必须有1个函数叫process_item

```python
def process_item(self,item,spider):
	return item
#必须返回item，此返回值会传给下一个管道的此函数继续处理
```

+ ##### 日志变量及日志级别(setting.py)

  ```python
  # 日志相关变量
  LOG_LEVEL = ''
  LOG_FILE = '文件名.log'
  
  # 日志级别
  5 CRITICAL :严重错误
  4 ERROR    :普通错误
  3 WARNING  :警告
  2 INFO     :一般信息
  1 DEBUG    :调试信息
  # 注意：只显示当前级别的日志和比当前级别日志更严重的
  ```

+ ##### 管道文件使用

```python
1. 在爬虫文件中为items.py中类做实例化，用爬下来的数据给对象赋值
	from ..items import MaoyanItem
	item = MaoyanItem()
2. 管道文件（pipelines.py）
3. 开启管道（setting.py）
	ITEM_PIPELINES = {'项目目录.pipelines.类名':优先级(数值越小优先级越高)}
```

+ ##### scrapy.Request()参数

  > :star:meta:scrapy.Request的时候会把meta交给调度器，与response一起返回
  >
  > scrapy.Request()参数 都会作为请求对象的属性

```
url
callback
meta : 传递数据，定义代理  
```

+ #### ::star:注意：涉及多级页面抓取

```
item对象创建在for循环里面
在中间二级，三级解析页面先不要给titem赋值，如需赋值，不能在for循环里(会导致重复赋值)
涉及到重复赋值把要提取的数据用meta传递到最后一级解析页面，统一给item对象赋值，统一yield item 到管道文件

```



### 数据持久化存储(MySQL)

```python
1. 在setting.py中定义相关变量
    #定义mysql相关变量
    MYSQL_HOST = 'localhost'
    MYSQL_USER = 'root'
    MYSQL_PWD = '123456'
    MYSQL_DB = 'maoyan'
    CHARSET = 'utf8'
 2. pipelines.py导入setting模块
 	def open_spider(self,spider):
        #爬虫开始执行1次，用于数据库连接
    def process_item(self,item,spider):
        #用于处理抓取的item数据
        return item
    def close_spider(self,spider):
        #爬虫结束时执行1次，用于断开数据库连接
 3. settings.py中添加此管道
     ITEM_PIPELINES = {'',200}
 
#注意：process_item()函数中一定要 return item
```



### 保存为csv、json文件

+ 命令格式

```python
scrapy crawl maoyan -o maoyan.csv
scrapy crawl maoyan -o maoyan.json

#settings.py中设置导出编码
FEED_EXPORT_ENCODING = 'utf-8'
```



### setting.py常用变量

```python
#1.设置日志级别
LOG_LEVEL = ''
#2.保存到日志文件(不在终端输出)
LOG_FILE = ''    LOG_FILE = 'maoyan.log'
#3.设置数据导出编码(主要针对于json文件)
FEED_EXPORT_ENCODING = ''     
#4.非结构化数据存储路径
IMAGES_STORE = '路径'
#5.设置User-Agent
USER-AGENT = ''
#6.设置最大并发数(默认为16)
CONCURRENT_REQUESTS = 32
#7.下载延迟时间(每隔多长时间请求一个网页)
#DOWNLOAD_DELAY 会影响 CONCURRENT_REQUESTS,不能使并发显现
#有CONCURRENT_REQUESTS，没有DOWNLOAD_DELAY：服务器会在同一时间收到大量请求
#有CONCURRENT_REQUESTS，有DOWNLOAD_DELAY：服务器不会在同一时间收到大量请求
DOWNLOAD_DELAY = 3
#8.请求头
DEFAULT_REQUESTS_HEADERS = {}
#9.添加项目管道
ITEM_PIPELINE = {}
#10.添加下载器中间件
DOWLOADER_MIDDLEWARES = {}
```



### 非结构化数据抓取

```python
1.新建：scrapy startproject 项目名
2. cd 项目文件夹
3. 新建爬虫文件：scrapy genspider 文件名 域名
4. 明确目标(items.py)
5. 写爬虫程序(文件名.py)
6. 管道文件重写get_media_requests

from scrapy.pipelines.images import ImagesPipeline
import scrapy
class SoPipeline(ImagesPipeline):
    def get_media_requests(self, item, info):
        #直接交给调度器入队列
        yield scrapy.Request(
            url=item['img_link'],
            meta={'title':item['img_title']}
        )

    # 重写file_path()方法：解决 路径+文件名 问题
    def file_path(self, request, response=None, info=None, *, item=None):
        title = request.meta['title']
        filename = title + '.jpg'
        return filename
    
 7. 设置setting.py
IMAGES_STORE='G:\\python\\So\\imge'
    
```



### scrapy.Request

```
#参数
1.url
2.callback
3.headers
4.meta :传递数据，定义代理
5.dont_filter :是否忽略域组限制 -默认False，检查allowed_domains['']
#request属性
1.request.url
2.request.hyeaders
3.request.meta
#response属性
1.response.url
2.response.text
3.response.body
4.response.meta
5.response.encoding

```



### 字符串方法总结

```python
1. strip()
2. split()
3. replace('','')
4. ''.join()
5. 字符串切片(正向切，反向切) : S[-10:]
```



### scrapy shell的使用

+ 基本使用

```python
# scrapy shell URL地址
1. request.url 			:请求URL地址
2. request.headers 		:请求头(字典)
3. request.meta 		:item数据传递，定义代理(字典)

4. response.text 		:字符串
5. response.body 		:bytes
6. response.xpath('')
```

+ scrapy.Request()参数

```
1.url
2.callback  ：指定解析函数
3.headers
4.meta :传递数据，定义代理
5.dont_filter :是否忽略域组限制
	默认False,检查allowed_domains['']

```



### 设置中间件(随机User-Agent)

#### 少量User-Agent切换

+ ##### 方法一

```python
# settings.py
USER_AGENT = ''
DEFAULT_REQUEST_HEADERS = {}
```

+ 方法二

```python
# spider
yield scrapy.Request(url,callback=函数名,headers={})
```

#### 大量User-Agent切换(中间件)

+ #### middlewares.py设置中间件

```python
1. 获取User-Agent
	#放法1 ：新建useragents.py,存放大量User-Agent，random模块随机切换
    #方法2 ：安装fake_useragent模块(sudo pip3 install fake_useragent)
    from fake_useragent import UserAgent
    ua_obj = UserAgent()
    ua = ua_obj.random
2. middlewares.py新建中间件类    
    class RandomUseragentMiddleware(object):
        def process_request(self,request,spider):
            ua = UserAgent()
            request.headers['User-Agent'] = ua.random
3. settings.py添加此下载器中间件
	DOWNLOADER_MIDDLEWARES = {'':优先级}
            
```



### 设置中间件(随机代理)

```python
class RandomProxyDownloaderMiddlerware(object):   
	def process_request(self,request,spider):
        request.meta['proxy'] = xxx
    def process_exception(self,request,exception,spider):
        return request
```



### item对象到底在何处创建?

```
1.一级页面：都可以，建议在for循环外
2.>=2级页面：for循环内
```



### scrapy - post请求

+ #### 方法+参数

```
scrapy.FormRequest(
	url=posturl,
    formdata=formdata,
    callback=self.parse
)
```

